{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d405f9a-9d82-4b57-9423-b6b4d206bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Spark session\n",
    "# import pandas as pd\n",
    "# from glob import glob\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark= SparkSession.builder \\\n",
    "#     .appName(\"ReadDataPyspark\") \\\n",
    "#     .config(\"spark.executor.memory\", \"6g\") \\\n",
    "#     .config(\"spark.driver.memory\", \"6g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"4\") \\\n",
    "#     .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "#     .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "#     .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\")\\\n",
    "#     .getOrCreate() # .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db73dc28-e6af-4141-b67d-7665f15016a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excel_file(spark,file_path, sheet_name=None, sheet_index=0):\n",
    "    # Validate input\n",
    "    if sheet_name is None and sheet_index is None:\n",
    "        raise ValueError(\"Either sheet_name or sheet_index must be provided.\")\n",
    "    \n",
    "    # Determine the dataAddress option based on sheet_name or sheet_index\n",
    "    if sheet_name:\n",
    "        data_address = f\"'{sheet_name}'!A1\"\n",
    "    else:\n",
    "        data_address = f\"'{sheet_index}'!A1\"\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "        .option(\"dataAddress\", data_address) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(file_path)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b384baa3-99be-4420-9ba2-e7528ad8a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def read_all_excel_files(spark,directory_path, sheet_name=None, sheet_index=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads all Excel files in a directory with partitioning for memory efficiency.\n",
    "    Returns:\n",
    "      pyspark.sql.DataFrame: A single DataFrame containing data from all Excel files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all file paths in the directory (assuming .xlsx extension)\n",
    "    file_paths = glob(f\"{directory_path}/*.xlsx\")\n",
    "    if not file_paths:\n",
    "        raise ValueError(f\"No Excel files found in directory {directory_path}.\")\n",
    "    \n",
    "    # Read each file and create a list of DataFrames\n",
    "    dfs = [read_excel_file(spark,file_path, sheet_name, sheet_index).withColumn(\"file_path\", lit(file_path)) for file_path in file_paths] #add path\n",
    "    \n",
    "    # Merge DataFrames in dfs into a single DataFrame\n",
    "    merged_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad7adec-a45a-4d2b-9eda-4a9eabfe07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(spark, file_path, delimiter=\",\"):\n",
    "    df = spark.read.csv(file_path, \n",
    "                        header=True, \n",
    "                        inferSchema=True,\n",
    "                        #schema=custom_schema, \n",
    "                        sep=delimiter, \n",
    "                        mode=\"DROPMALFORMED\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1727b49-6003-46cb-bb83-8dbbb24cec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_large_file(spark, file_path, delimiter=\",\"):\n",
    "    df = (\n",
    "    spark.read.format(\"csv\") \n",
    "    .option(\"delimiter\", delimiter) \n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\") \n",
    "    .option(\"mode\", \"DROPMALFORMED\") \n",
    "      #.option(\"schema\", custom_schema) \n",
    "    .load(file_path)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc28eb65-5bb9-4db3-b428-3dd4e3d73d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def rename_columns(df):\n",
    "  \"\"\"\n",
    "  Renames columns in a DataFrame with underscores replacing spaces/special characters,\n",
    "  ensuring only one underscore for consecutive replacements.\n",
    "\n",
    "  Args:\n",
    "      df (pyspark.sql.DataFrame): The DataFrame to rename columns in.\n",
    "\n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame: The DataFrame with renamed columns.\n",
    "  \"\"\"\n",
    "  for column in df.columns:\n",
    "    # Use regular expression to replace consecutive spaces, newlines, etc. with single underscore\n",
    "    #new_column = re.sub(r\"\\s+|\\n+\", \"_\", column)\n",
    "    new_column = re.sub(r'\\s+|\\n+|-+', '_', column)\n",
    "    # Replace other special characters with underscore\n",
    "    #new_column = new_column.replace(\"(\", \"_\").replace(\")\", \"_\").replace(\"-\", \"_\")\n",
    "    #new_column = new_column.replace(\"-\", \"_\")\n",
    "    df = df.withColumnRenamed(column, new_column)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2999827b-c89b-47bc-9869-9287c598f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sql(spark, spark_df, host_url, tb_name, user, pw):\n",
    "    \"\"\"\n",
    "    Writes a Spark DataFrame to a SQL database.\n",
    "\n",
    "    Parameters:\n",
    "        spark (SparkSession): The Spark session object.\n",
    "        spark_df (DataFrame): The Spark DataFrame to be written to SQL.\n",
    "        host_url (str): The JDBC URL for the SQL database.\n",
    "        tb_name (str): The target table name in the SQL database.\n",
    "        user (str): The username for the SQL database.\n",
    "        pw (str): The password for the SQL database.\n",
    "        mode (str): Save mode (default is \"overwrite\"). Other options include \"append\", \"ignore\", \"error\".\n",
    "        batch_size (int): The number of records to write in each batch (default is 60,000).\n",
    "        num_partitions (int, optional): Number of partitions to use for writing. Defaults to the current partition count.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine an appropriate number of partitions (adjust based on your dataset size and cluster capacity)\n",
    "        num_partitions = spark_df.rdd.getNumPartitions()\n",
    "        \n",
    "        # Increase batch size\n",
    "        batch_size = 60000  #\n",
    "       # Using append mode can sometimes be faster\n",
    "        spark_df.repartition(num_partitions).write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", host_url) \\\n",
    "            .option(\"dbtable\", tb_name) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", pw) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .option(\"batchsize\", batch_size) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"Successfully loaded data into table: {tb_name}\")\n",
    "    \n",
    "    except ValueError as error:\n",
    "        print(f\"ValueError occurred during SQL write: {error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
