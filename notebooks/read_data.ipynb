{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaa9b1a-cee0-4cec-b31b-cefa13d152d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef17c51f-483d-4a6b-9680-0edfdbd0a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run reuse_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efe981d-f52d-431a-9312-a54474c1c679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/profshabangu/Downloads/data engineering/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b40103-a1e0-4370-9e14-a4b74900cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "#from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45563448-1928-4611-af22-ac2740061ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/profshabangu/Downloads/Scraping%20Shoping%20websites/venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/profshabangu/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/profshabangu/.ivy2/jars\n",
      "com.crealytics#spark-excel_2.12 added as a dependency\n",
      "com.microsoft.sqlserver#mssql-jdbc added as a dependency\n",
      "com.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-078a2640-7357-491e-ac49-4b124ef2647e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.crealytics#spark-excel_2.12;0.13.7 in central\n",
      "\tfound org.apache.poi#poi;4.1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.13 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.4 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound com.zaxxer#SparseBitSet;1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.2 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound com.norbitltd#spoiwo_2.12;1.8.0 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.12;1.3.0 in central\n",
      "\tfound com.github.pjfanning#excel-streaming-reader;2.3.6 in central\n",
      "\tfound com.github.pjfanning#poi-shared-strings;1.0.4 in central\n",
      "\tfound com.h2database#h2;1.4.200 in central\n",
      "\tfound org.apache.commons#commons-text;1.8 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.9 in central\n",
      "\tfound xml-apis#xml-apis;1.4.01 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.commons#commons-compress;1.20 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;9.2.1.jre8 in central\n",
      "\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.1.0 in central\n",
      ":: resolution report :: resolve 1317ms :: artifacts dl 53ms\n",
      "\t:: modules in use:\n",
      "\tcom.crealytics#spark-excel_2.12;0.13.7 from central in [default]\n",
      "\tcom.github.pjfanning#excel-streaming-reader;2.3.6 from central in [default]\n",
      "\tcom.github.pjfanning#poi-shared-strings;1.0.4 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.h2database#h2;1.4.200 from central in [default]\n",
      "\tcom.microsoft.azure#spark-mssql-connector_2.12;1.1.0 from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;9.2.1.jre8 from central in [default]\n",
      "\tcom.norbitltd#spoiwo_2.12;1.8.0 from central in [default]\n",
      "\tcom.zaxxer#SparseBitSet;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.13 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.20 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.9 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.8 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.2 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.12;1.3.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\txml-apis#xml-apis;1.4.01 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.commons#commons-compress;1.19 by [org.apache.commons#commons-compress;1.20] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   23  |   0   |   0   |   1   ||   22  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-078a2640-7357-491e-ac49-4b124ef2647e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 22 already retrieved (0kB/24ms)\n",
      "24/08/25 16:58:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadandWriteApp\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"com.crealytics:spark-excel_2.12:0.13.7,\" +\n",
    "            \"com.microsoft.sqlserver:mssql-jdbc:9.2.1.jre8,\" +\n",
    "            \"com.microsoft.azure:spark-mssql-connector_2.12:1.1.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f1bd9e-541e-40ae-bf11-73664b90402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: \\nif more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, \\nan OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended \\nperiod of time while making little or no progress because the heap is too small.\\nIf necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: \n",
    "if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, \n",
    "an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended \n",
    "period of time while making little or no progress because the heap is too small.\n",
    "If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bda4159-4eeb-4f63-aa8b-99001dd3229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../retailer_data/Spar/Insights 2024xlsx.xlsx\n"
     ]
    }
   ],
   "source": [
    "spar_dir = \"../retailer_data/Spar\"\n",
    "file_paths = glob(f\"{spar_dir}/*.xlsx\")\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f567eec-a03e-43c0-b0fa-d35c4ee20ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Month_Yr=datetime.datetime(2024, 3, 1, 0, 0), Region='NR', Cluster='CL1', Barcode='6009814580351', Product___Size='MORINGA TREE OF LIFE - 500MLT', Segment='ICED TEA', Brand='MORINGA', Store='TSHAKHUMA SAVEMOR', Format='SAVEMOR', Sales_(R)_2024=309.69, Sales_(R)_2023=499.5, Units_2024=31.0, Units_2023=50.0, Sales_Gwth=-0.38, Units_Gwth=-0.38, file_path='../retailer_data/Spar/Insights 2024xlsx.xlsx')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read Spar Data:\n",
    "spar_all_df = read_all_excel_files(spark,spar_dir, sheet_index=0)\n",
    "spar_all_df = rename_columns(spar_all_df)\n",
    "spar_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b7f6e-f2b4-4d27-9205-9a542328f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ba06d7-731e-4708-ad1b-1e3c5b4d5e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(spar_all_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf50f0c9-d04a-44f4-aaa9-0a97265d8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "# from functools import reduce\n",
    "# from pyspark.sql.functions import lit\n",
    "\n",
    "def read_excel_files_count(spark, directory_path, sheet_name=None, sheet_index=0):\n",
    "  \"\"\"\n",
    "  Reads all Excel files in a directory using PySpark.\n",
    "\n",
    "  Args:\n",
    "      spark (SparkSession): The SparkSession object.\n",
    "      directory_path (str): The path to the directory containing Excel files.\n",
    "      sheet_name (str, optional): The name of the sheet to read (default: None).\n",
    "      sheet_index (int, optional): The zero-based index of the sheet to read (default: 0).\n",
    "\n",
    "  Returns:\n",
    "      list: A list of DataFrames, each corresponding to a single Excel file.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Get all file paths in the directory\n",
    "  file_paths = glob(f\"{directory_path}/*.xlsx\")  # Assuming .xlsx extension\n",
    "\n",
    "  # Read each file using the original function and append to a list\n",
    "  df = None\n",
    "  for file_path in file_paths:\n",
    "    df = read_excel_file(spark, file_path, sheet_name, sheet_index)\n",
    "    print(df.count())\n",
    "  #   if df is None:\n",
    "  #     df = df_temp\n",
    "  #   else:\n",
    "  #     df = df.union(df_temp)\n",
    "\n",
    "  #return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d258dba-319d-4f65-9f88-5507cf01cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a69bf5f-d4d4-42e0-93b0-297cf7521fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailers = [\n",
    "    \"Spar\"\n",
    "    ,\"PNP\"\n",
    "    ,\"Checkers\"\n",
    "    #,\"Woolworths\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8a0263b-7ba0-4dfb-9fac-b7776322a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('Calendar Month/Year', DoubleType(), True), StructField('Site Name', StringType(), True), StructField('Site', StringType(), True), StructField('Article Name', StringType(), True), StructField('Article', DoubleType(), True), StructField('Customers Select. Per.', LongType(), True), StructField('Tot. Customers Select. Per.', LongType(), True), StructField('Customers Pen. Select. Per', DoubleType(), True), StructField('Sales Units Select. Per.', LongType(), True), StructField('Measure', StringType(), True), StructField('Sales Unit Part. Select. Per', DoubleType(), True), StructField('Net Sales Value Select. Per.', LongType(), True), StructField('Measure.1', StringType(), True), StructField('Net Sales Value Part. Select. Per.', DoubleType(), True), StructField('Transaction Select. Per.', LongType(), True), StructField('Transaction Pen Select Per.', DoubleType(), True), StructField('Tot. Transaction Select. Per.', LongType(), True), StructField('Avg Customer Freq. Select. Per', DoubleType(), True), StructField('Avg. Customer Spend Select. Per.', DoubleType(), True), StructField('Measure.2', StringType(), True), StructField('Avg. Transaction Spend Select. Per.', DoubleType(), True), StructField('Measure.3', StringType(), True), StructField('Avg. Transaction Units Select. Per', DoubleType(), True), StructField('Avg. Unit Price Select. Per', DoubleType(), True), StructField('Measure.4', StringType(), True), StructField('Distribution Select. Per.', LongType(), True), StructField('% Distribution Select. Per.', DoubleType(), True)])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "# Get the Spark dataframe schema\n",
    "schema = df_spark.schema\n",
    "\n",
    "# Convert the Spark schema to a Python structfield schema\n",
    "python_schema = StructType([StructField(field.name, field.dataType, field.nullable) for field in schema.fields])\n",
    "\n",
    "# Print the Python structfield schema\n",
    "print(python_schema)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
